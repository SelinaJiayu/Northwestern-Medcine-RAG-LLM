{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os \n",
    "from pathlib import Path \n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import torch\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>question_len</th>\n",
       "      <th>answer_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is radiation therapy?</td>\n",
       "      <td>Radiation therapy (also called radiotherapy) i...</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is radiation therapy given?</td>\n",
       "      <td>Radiation therapy can be external beam or inte...</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who gets radiation therapy?</td>\n",
       "      <td>Many people with cancer need treatment with ra...</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does radiation therapy do to cancer cells?</td>\n",
       "      <td>Given in high doses, radiation kills or slows ...</td>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How long does radiation therapy take to work?</td>\n",
       "      <td>Radiation therapy does not kill cancer cells r...</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Question  \\\n",
       "0                       What is radiation therapy?   \n",
       "1                  How is radiation therapy given?   \n",
       "2                      Who gets radiation therapy?   \n",
       "3  What does radiation therapy do to cancer cells?   \n",
       "4    How long does radiation therapy take to work?   \n",
       "\n",
       "                                              Answer  question_len  answer_len  \n",
       "0  Radiation therapy (also called radiotherapy) i...             4          48  \n",
       "1  Radiation therapy can be external beam or inte...             5          65  \n",
       "2  Many people with cancer need treatment with ra...             4          34  \n",
       "3  Given in high doses, radiation kills or slows ...             8          87  \n",
       "4  Radiation therapy does not kill cancer cells r...             8          35  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path.cwd().parent / 'data'\n",
    "file_name = 'prelim_data_cleaned.csv'\n",
    "data = pd.read_csv(data_dir / file_name)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Retrieval Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedder(embedder_config, device):\n",
    "    embedder_params = embedder_config['params']\n",
    "    if embedder_config['backend'] == \"HF\":\n",
    "        embedder = HuggingFaceEmbeddings(\n",
    "            model_name=embedder_params['model_name'],\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs=embedder_params.get('encode_kwargs', {})\n",
    "        )\n",
    "        print(f\"Embedder Initialized with {embedder_params['model_name']}\")\n",
    "    elif embedder_config['backend'] == \"OLLAMA\":\n",
    "        embedder = OllamaEmbeddings(model=embedder_params['model_name'])\n",
    "        print(f\"Embedder Initialized with {embedder_params['model_name']}\")\n",
    "    else:\n",
    "        raise NotImplementedError(\"Embedder backend not supported\")\n",
    "    return embedder\n",
    "\n",
    "\n",
    "def init_vectorstore(embedder):\n",
    "    # Create the FAISS index for storing embeddings\n",
    "    embedding_size = len(embedder.embed_query(\"hello world\"))  # Example to get embedding size\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    \n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={}\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text for embedding by normalizing and cleaning.\n",
    "    :param text: input string to preprocess\n",
    "    :return: cleaned and preprocessed string\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "def create_documents_from_questions(questions, question_ids):\n",
    "    for index in range(len(questions)):\n",
    "        questions[index] = preprocess_text(questions[index])\n",
    "    \n",
    "    documents = []\n",
    "    for text, doc_id in zip(questions, question_ids):\n",
    "        _ = Document(\n",
    "            page_content=text,\n",
    "            metadata={\"id\" : doc_id}\n",
    "        )\n",
    "        documents.append(_)        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define Configs and Settings For Retrieval\n",
    "SCORE_THRESHOLD = 0.5\n",
    "TOPK=5\n",
    "\n",
    "EMBEDDER_CONFIG = {\n",
    "    \"backend\" : \"HF\", \n",
    "    \"params\" : {\n",
    "        \"model_name\" : \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"encode_kwargs\" : {'normalize_embeddings': False}\n",
    "    }\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder Initialized with sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Intialize Embedder and VectorStore\n",
    "embedder = init_embedder(embedder_config=EMBEDDER_CONFIG, device=device)\n",
    "vectorstore = init_vectorstore(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 Documents added to Vector Store\n"
     ]
    }
   ],
   "source": [
    "# Add Documents to VectorStore\n",
    "documents = create_documents_from_questions(data.Question.tolist(), data.index.tolist())\n",
    "vectorstore.add_documents(documents=documents, ids=data.index.tolist())\n",
    "print(f\"{len(documents)} Documents added to Vector Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Functions as Runnables to use in retrieval chain\n",
    "@chain\n",
    "def retriever(query: dict) -> List[Document]:\n",
    "    \"\"\"Custom Retriever Logic to filter based on SIM THRESHOLD\"\"\"\n",
    "    query = preprocess_text(query.get('query',''))\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_relevance_scores(query, k=TOPK))\n",
    "    result = []\n",
    "    for doc, score in zip(docs, scores):\n",
    "        if score > SCORE_THRESHOLD:\n",
    "            doc.metadata[\"score\"] = score\n",
    "            result.append(doc)\n",
    "    return result\n",
    "\n",
    "@chain\n",
    "def format_retrieved_docs(documents: List[Document]) -> str:\n",
    "    \"\"\"Context Formatter\"\"\"\n",
    "    docs = []\n",
    "    for doc in documents:\n",
    "        score = doc.metadata.get('score')\n",
    "        doc_index = doc.metadata.get('id')\n",
    "        answer = data.iloc[doc_index].Answer\n",
    "        docs.append((doc.page_content, answer, score))\n",
    "\n",
    "    context_text = \"\\n\".join([f\"Q: {ctx[0]}\\nA: {ctx[1]}\" for ctx in docs])\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_chain = retriever | format_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Chain to fill User's InfoBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'know_base': KnowledgeBase(name='unknown', age='unknown', gender='unknown', query='What is radiation therapy and how can it be used to treat cancer?', summary='User asked about radiation therapy and its use in treating cancer.', response='Radiation therapy, also known as radiotherapy, is a type of cancer treatment that uses high-energy rays to kill or shrink cancer cells. It can be used alone or in combination with other treatments, such as surgery or chemotherapy. Radiation therapy works by damaging the DNA of cancer cells, which prevents them from growing and dividing.'), 'input': 'What is radiation therapy and how can it be used to treat cancer?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = \n",
    "    ({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    name: str = Field(default=\"unknown\", description=\"Name of the user, first name and last name\")\n",
    "    age : str = Field(default=\"unknown\", description=\"Age of the user\")\n",
    "    gender: str = Field(default=\"unknown\", description=\"Gender of the user\")\n",
    "    query: str = Field(default=\"unknown\", description=\"Detailed User's Query to answer. It should be framed in Question format.\")\n",
    "    summary: str = Field('unknown', description=\"Running summary of conversation. Update this with new input\")\n",
    "    response: str = Field('unknown', description=\"An ideal response to the user based on their new message\")\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are chatting with a user. The user just responded ('input'). Please update the knowledge base.\"\n",
    "    \"Record your response in the 'response' tag to continue the conversation.\"\n",
    "    \"Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
    "    \"Update the entries frequently to adapt to the conversation flow.\"\n",
    "    \"\\n{format_instructions}\"\n",
    "    \"\\n\\nOLD KNOWLEDGE BASE: {know_base}\"\n",
    "    \"\\n\\nNEW MESSAGE: {input}\"\n",
    "    \"\\n\\nNEW KNOWLEDGE BASE:\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = OllamaLLM(model='llama3.1:70b', temperature=0.4)\n",
    "instruct_llm = llm #| StrOutputParser\n",
    "extractor = RExtract(KnowledgeBase, instruct_llm, parser_prompt)\n",
    "info_update = RunnableAssign({'know_base' : extractor})\n",
    "\n",
    "## Initialize the knowledge base and see what you get\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"What is radiation therapy and how can it be used to treat cancer?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "operator.itemgetter(KnowledgeBase(name='unknown', age='unknown', gender='unknown', query='What is radiation therapy and how can it be used to treat cancer?', summary='User asked about radiation therapy and its use in treating cancer.', response='Radiation therapy, also known as radiotherapy, is a type of cancer treatment that uses high-energy rays to kill or shrink cancer cells. It can be used alone or in combination with other treatments, such as surgery or chemotherapy. Radiation therapy works by damaging the DNA of cancer cells, which prevents them from growing and dividing.'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "itemgetter(state.get('know_base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def extract_query(state):\n",
    "    return state.get('know_base').query\n",
    "\n",
    "internal_chain = (\n",
    "    info_update\n",
    "    | RunnableAssign({\"query\" : extract_query})\n",
    "    | RunnableAssign({'context' : ret_chain})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"I am Ayush Agarwal. Who am I?\"\n",
    "state = internal_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['know_base', 'input', 'query', 'context', 'output'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['output'] = \"\"\n",
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a chatbot for NU Medicine, and you are providing users cancer treatment assistance.\"\n",
    "        \" Please chat with them! Stay concise and clear!\"\n",
    "        \" Your running knowledge base is: {know_base}.\"\n",
    "        \" This is for you only; Do not mention it!\"\n",
    "        \" \\nUsing that, we retrieved the following: {context}\\n\"\n",
    "        \"\\nHere is the query that user want to get answered: {query}\\n\"\n",
    "        \"\\nKeep asking follow up questions, until user provides basic info like name, age and gender\\n\"\n",
    "        \"\\nModerate your tone according to user's age and gender\\n\"\n",
    "        \"If they provide info and the retrieval fails, ask to confirm their name, age, gender and query\"\n",
    "        \" Do not ask them any other personal info.\"\n",
    "    )),\n",
    "    (\"assistant\", \"{output}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "external_chain = external_prompt | OllamaLLM(model='llama3.1:70b', temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Ayush! You've just told me your name, but I don't have any information about you beyond that. Could you please provide more context or clarify what you mean by 'who am I'? Are you looking for general information about yourself or is there something specific you're trying to understand?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "\n",
    "    ## Pulling in, updating, and printing the state\n",
    "    global state\n",
    "    state['input'] = message\n",
    "    state['history'] = history\n",
    "    state['output'] = \"\" if not history else history[-1][1]\n",
    "\n",
    "    ## Generating the new state from the internal chain\n",
    "    state = internal_chain.invoke(state)\n",
    "    #print(\"State after chain run:\")\n",
    "    #pprint({k:v for k,v in state.items() if k != \"history\"})\n",
    "    \n",
    "    ## Streaming the results\n",
    "    buffer = \"\"\n",
    "    for token in external_chain.stream(state):\n",
    "        buffer += token\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=8):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "chat_history = [[None, \"Hello! I'm your NU Medicine Agent! How can I help you?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "# queue_fake_streaming_gradio(\n",
    "#     chat_stream = chat_gen,\n",
    "#     history = chat_history\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://8a67ac0cb0de98e8cf.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8a67ac0cb0de98e8cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://8a67ac0cb0de98e8cf.gradio.live\n"
     ]
    }
   ],
   "source": [
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "chatbot = gr.Chatbot(value=[[None, \"Hello! I'm your NU Medicine Agent! How can I help you?\"]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue().launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e07611189b1792198ccfb17104255956e1e903ed7e83ac798d2f755ad9013bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
